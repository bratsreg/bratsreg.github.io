<!doctype html>
<html>

<head>
  <title>BraTS-Reg</title>
  <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1">
  <link href="css/frame.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/controls.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/custom.css" media="screen" rel="stylesheet" type="text/css" />
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <!script src="js/menu.js"></script>
  <style>
    .menu-index {
      color: rgb(255, 255, 255) !important;
      opacity: 1 !important;
      font-weight: 700 !important;
    }
  </style>
</head>

<body>
  <!div class="menu-container"></div>
  <div class="content-container">
    <!-------------------------------------------------------------------------------------------->
    <div class="banner" style="background: url('img/brats-reg1.png') no-repeat center; background-size: cover; height: 260px;">
      <div class="banner-table flex-column">
      </div>
    </div>
    <!-------------------------------------------------------------------------------------------->
    <div class="banner">
      <div class="banner-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin-small">Brain Tumor Sequence Registration (BraTS-Reg) Challenge</h2>
            <p class="text">
              Establishing Correspondence between Pre-Operative and Follow-up MRI
            </p>
          </div>
        </div>
      </div>
    </div>
    <!-------------------------------------------------------------------------------------------->
    <div class="content">
      <div class="content-table flex-column">
        <!-------------------------------------------------------------------------------------------->
        <!--Start background-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h2>Background</h2>
            <hr>
            <p class="text">
              Registration of Magnetic Resonance Imaging (MRI) scans containing pathologies is challenging due to tissue appearance changes, and still an unsolved problem. We organize the first Brain Tumor Sequence Registration (BraTS-Reg) challenge, focusing on estimating correspondences between baseline pre-operative and follow up scans of the same patient diagnosed with a brain glioma. The BraTS-Reg  challenge intends to establish a benchmark environment for deformable registration algorithms. The dataset associated with this challenge comprises de-identified multi-institutional multi-parametric MRI (mpMRI) data, curated for each scan’s size and resolution, according to a common anatomical template. The clinical experts of our team have generated extensive annotations of landmarks points within the scans. The “training data” along with these ground truth annotations will be released to participants to design their registration methods, whereas annotations of the “validation” and “test” data will be withheld by the organizers and used to evaluate the containerized algorithms of the participants. We will conduct the quantitative evaluation of the submitted algorithms using several metrics, such as Median Absolute Error and Robustness.  
            </p>
          </div>
        </div>
        <!--End background-->
        
        <!-------------------------------------------------------------------------------------------->
        <!-------------------------------------------------------------------------------------------->


        <!--Start Important Dates-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h2>Important Dates</h2>
            <hr>
             <style type="text/css">
              .tg  {border-collapse:collapse;border-spacing:0;}
              .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
                overflow:hidden;padding:10px 5px;word-break:normal;}
              .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
                font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
              .tg .tg-lboi{border-color:inherit;text-align:left;vertical-align:middle}
              .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}
              .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
              </style>
              <table class="tg">
              <thead>
                <tr>
                  <th class="tg-0pky">8 Apr 2022</th>
                  <th class="tg-lboi">Registration opens for BraTS-Reg @ MICCAI.</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td class="tg-0pky">8 Apr 2022</td>
                  <td class="tg-0pky">Training &amp; Validation data release.</td>
                </tr>
                <tr>
                  <td class="tg-0pky">8 Apr 2022</td>
                  <td class="tg-0pky">Evaluation Platform goes live!</td>
                </tr>
                <tr>
                  <td class="tg-0pky">2 May 2022</td>
                  <td class="tg-0pky">Leaderboard goes live! (https://www.cbica.upenn.edu/BraTSReg22/)</td>
                </tr>
                <tr>
                  <td class="tg-0pky">17 July 2022</td>
                  <td class="tg-0pky">Submission of short paper reporting method &amp; results.</td>
                </tr>
                <tr>
                  <td class="tg-0pky">24 July 2022</td>
                  <td class="tg-0pky">Submission of containerized algorithm.</td>
                </tr>
                <tr>
                  <td class="tg-0pky">18 Aug 2022</td>
                  <td class="tg-0pky">Contacting top-ranked teams to prepare oral presentations.</td>
                </tr>
                <tr>
                  <td class="tg-0pky">18-22 Sep 2022</td>
                  <td class="tg-0pky">Announcement of top-ranked teams at MICCAI.</td>
                </tr>
                <tr>
                  <td class="tg-0pky">30 Oct 2022</td>
                  <td class="tg-0pky">Camera-ready submission of extended papers for inclusion in the associated workshop proceedings.</td>
                </tr>
                <tr>
                  <td class="tg-c3ow" colspan="2"><span style="font-weight:700;color:inherit">(All deadlines are for 23:59 Eastern Time)</span></td>
                </tr>
              </tbody>
              </table>
          </div>
        </div>
        <!--End Important Dates-->
        
        <!-------------------------------------------------------------------------------------------->
        <!-------------------------------------------------------------------------------------------->

        <!--Start Clinical Relevance-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h2>Clinical Relevance</h2>
            <hr>
            <p class="text">
              Registration of baseline pre-operative (treatment-naïve) and follow-up brain tumor MRI scans is challenging, yet a clinically important task for a multitude of reasons. Brain tissue shows heavy deformations induced by the apparent tumor (also known as mass effect) that following its resection are relaxed due to the relieving pressure from the resected tissue. Such deformations affect the whole brain (including the lateral ventricles) and are not limited to the vicinity of the tumor. This is particularly important as the relationship of the tumor to the lateral ventricles and the deformations to the rest of the brain tissue are important factors in prognosis and treatment planning. Further changes in the peritumoral edematous/infiltrated tissue, potential tumor recurrence, as well as treatment related changes, also affect the brain tissue elasticity. The resected tissue/tumor also relates to missing correspondences, and inconsistent intensity profiles between the follow up and the baseline pre-operative scans.  
            </p>
            <p class="text">
              Taking all the above into consideration, finding spatial correspondences between two longitudinal scans of brain tumor patients, i.e., the registration between the baseline pre-operative and follow-up MRI scans, can advance our mechanistic understanding for these tumors. Specifically, for tumor infiltration and potential recurrence, further contributing in the generation of predictive modelling for related pathophysiological processes, but also in understanding biophysical dynamic and plasticity characteristic of brain tissues, as well as for neurosurgical planning.
            </p>            
          </div>
        </div>
        <!--End Clinical Relevance-->
        
        <!-------------------------------------------------------------------------------------------->
        <!-------------------------------------------------------------------------------------------->

        <!--Start Task & Evaluation-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h2>Task & Evaluation</h2>
            <hr>
            <p class="text">
              The registration between pre-operative and follow-up MRI scans of brain glioma patients, is important yet challenging task. In this challenge, participants are invited to develop deformable image registration algorithm by using the provided clinically acquired training data and the annotations done by our expert clinical neuroradiologists.
            </p>
            <p class="text">
              The evaluation of the registration between the two scans will be based on manually seeded landmarks (ground truth) in both the pre-operative and the follow-up scans. The performance will be quantitatively evaluated in terms of Median Absolute Error (MAE) and Robustness.
            </p>
          </div>
        </div>
        <!--End Task & Evaluation-->
        
        <!-------------------------------------------------------------------------------------------->
        <!-------------------------------------------------------------------------------------------->


        <!--Start Data-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h2>Data</h2>
            <hr>
            <p class="text">
              We have identified, curated, and pre-processed retrospective multi-institutional data. The data comprises of pairs of pre-operative baseline and follow-up MRI brain scans (each pair being of the same patient) diagnosed and treated for glioma. The exact multi-parametric MRI (mpMRI) sequences of each timepoint are i) native (T1) and ii) contrastenhancedT1-weighted (T1-CE), iii) T2-weighted and iv) T2Fluid Attenuated Inversion Recovery (FLAIR).
            </p>
            <p class="text">
              In training phase, participants will be provided with the ground truth annotations along with the MRI data. The ground truth consists of location of some unique landmark points found in baseline scan and their corresponding locations in follow-up scan to develop the deformable registration algorithms.
            </p>
            <p class="text">
              These landmarks are defined on anatomical markers such as blood vessel bifurcations, the anatomical shape of the cortex, and anatomical landmarks of the midline of the brain. The total number of landmarks vary from case to case and across all cases in the range of 6-50 per scan.
            </p>
            <p class="text">
              The validation data will be provided to the participants as scan pairs of baseline and follow-up with landmarks provided only for the follow-up scan. Participants will submit coordinates of warped landmark locations in the baseline scan. Also, they will be called to upload their method in a containerized way for evaluation on testing data.
            </p>
          </div>
        </div>
        <!--End Data-->
        
        <!-------------------------------------------------------------------------------------------->
        <!-------------------------------------------------------------------------------------------->

        <!--Start Participation Timeline Summary-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h2>Participation Timeline Summary</h2>
            <hr>
            <h3>
            <p class="text">
              Training & Validation Data availability (April 2022) 
            </p> 
            </h3> 
            <p class="text">
              Register for the BraTS-Reg challenge, to get access to the NIFTI, skull-stripped, and annotated training data. An independent set of validation scans will be made available to the participants, with the intention to allow them assess the generalizability of their methods on unseen data, via the official evaluation platform. Note that the training data will include its corresponding ground truth, whereas the validation data will have its ground truth data held by the organizers at all times. 
            </p>
            
            <h3>
            <hr>
            <p class="text">
              Short Paper submission deadline (July 2022)  
            </p> 
            </h3>            
            <p class="text">
              Participants will have to evaluate their methods on the training and validation datasets, and submit short paper describing their method and results. The organizers will review the paper for sufficient details required to understand and reproduce the algorithm. The challenge participants will be given a chance with the option to extend their individual papers, and hence publish their methods in Springer LNCS proceedings. 
            </p>

            <h3>
            <hr>
            <p class="text">
              Testing phase (July 2022) 
            </p> 
            </h3> 
            <p class="text">
              Participants will need to submit their method in a containerized form. Note that only participants that have submitted a short paper will be considered in the testing phase. The BraTS-Reg test data will not be made available to the participating teams at any point during the lifecycle of this challenge. We will communicate more details on how to create the expected containers soon. 
            </p>

            <h3>
            <hr>
            <p class="text">
             Oral Presentations at MICCAI (August 2022)  
            </p> 
            </h3>  
            <p class="text">
              The top-ranked participants of the testing phase will be contacted by August to prepare slides for orally presenting their method in MICCAI 2022.  The final results of the challenge will also be reported at MICCAI. 
            </p>

            <h3>
            <hr>
            <p class="text">
             Joint post-conference journal paper 
            </p> 
            </h3>  
            <p class="text">
              Finally, we intend to coordinate a journal meta-analysis manuscript in one of the reputed journals in the domain by extending the preprint appropriately to describe the challenge design, data, clinical relevance, and summarizing the results and insights of the challenge. 
            </p>
          </div>
        </div>
        <!--End Participation Timeline Summary-->
        <!-------------------------------------------------------------------------------------------->
        <!-------------------------------------------------------------------------------------------->


        <!--Registration / Data Request-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h2>Registration / Data Request</h2>
            <hr>
            <p class="text no-bottom-margin">
              Challenge data may be used for all purposes, provided that the challenge is appropriately referenced using the citation given at the bottom of this page.
            </p>
            <p class="text no-bottom-margin">
              To request the training and the validation data of the BraTS-Reg 2022 challenge, please follow the registration steps below. Please note that the i) training data includes ground truth annotations, ii) validation data does not include annotations, and iii) testing data are not available to the public or to the challenge participants.
            </p>
            <p class="text no-bottom-margin">
              To register for the challenge and request the data of the BraTS-Reg challenge, please follow the steps below:
            </p>            
            <ul>
              <li>
                <p class="text-small-margin">
                  Create an account in CBICA's Image Processing Portal (ipp.cbica.upenn.edu) and wait for its approval. Note that a confirmation email will be sent so make sure that you also check your Spam folder.
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  Once your IPP account is approved, login to ipp.cbica.upenn.edu and then click on the application "BraTS-Reg 2022: Registration and Data Request", under the "Brain Tumor Sequence Registration (BraTS-Reg) Challenge" group.
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  Fill in the requested details and press "Submit Job".
                </p>
              </li>
             <li>
              <p class="text-small-margin">
                Once your request is recorded, you will receive an email pointing to the "results" of your submitted job. You need to login to IPP, access the "Results.zip" file, in which you will find the file “REGISTRATION_STATUS.txt” that will provide the links to download the BraTS-Reg data. The training data will include for each subject the 4 structural modalities and ground truth landmarks, whereas the validation data will include only the 4 modalities.
              </p>
            </li>
            </ul>
            <p class="text no-bottom-margin">
              Please note that you are expected to use CBICA's IPP to evaluate your method against the ground truth labels of the validation datasets. 
              You are free to use and/or refer to the BraTS-Reg datasets in your own research, provided that you always cite the following manuscript:
            </p>
            <p class="text no-bottom-margin">
              [1] B.Baheti, D.Waldmannstetter, S.Chakrabarty, et al., "The Brain Tumor Sequence Registration Challenge: Establishing Correspondence between Pre-Operative and Follow-up MRI scans of diffuse glioma patients", arXiv preprint 2112.06979 (2021)
            </p>            
          </div>
        </div>
        <!-- End of Registration / Data Request-->
        <!-------------------------------------------------------------------------------------------->
        <!-------------------------------------------------------------------------------------------->

        <!--Start Leaderboard-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h2>Leaderboard</h2>
            <hr>
            <p class="text">
              The leaderboard of Training and Validation phase of BraTSReg challenge is now live at: https://www.cbica.upenn.edu/BraTSReg22/ . This is the unranked leaderboard including the numbers of cases each team has uploaded and the latest evaluation metrics for these cases.
            </p>
            <p class="text">
              Taking all the above into consideration, finding spatial correspondences between two longitudinal scans of brain tumor patients, i.e., the registration between the baseline pre-operative and follow-up MRI scans, can advance our mechanistic understanding for these tumors. Specifically, for tumor infiltration and potential recurrence, further contributing in the generation of predictive modelling for related pathophysiological processes, but also in understanding biophysical dynamic and plasticity characteristic of brain tissues, as well as for neurosurgical planning.
            </p>            
          </div>
        </div>
        <!--End Leaderboard-->
        <!-------------------------------------------------------------------------------------------->
        <!-------------------------------------------------------------------------------------------->

        <!--Start Submission of Containers for Testing Phase-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h2>Submission of Containers for Testing Phase</h2>
            <hr>
            <p class="text">
              The test data will not be released publicly and the participants need to submit their method in a containerized form (Singularity) to be evaluated by the organizers on the hidden test data. The specific instructions for creating singularity containers are provided at: https://github.com/satrajitgithub/BraTS_Reg_submission_instructions
            </p>
            <p class="text">
              Please note that there are additional outputs required from containers, along with the warped landmark locations and determinant of Jacobian, as detailed in the container submission instructions. We would suggest you to submit the containers before the deadline, if you can. This will allow us to check if there are any issues with the containers and can inform you accordingly.
            </p>  
             <p class="text">
               Once the container is submitted as per the above instructions, please make sure to email the organizers the exact name of the container that you have uploaded to the “Sylabs Cloud Library”: library://<your_sylab_username>/BraTS_Reg/BraTS_Reg_<team_name>[:<tag>] 
             </p>
             <p class="text">
               The challenge organizers will run all the methods on the test data using own computational infrastructure to confirm reproducibility of results. Please note that your containers will be evaluated only if you have submitted the accompanying manuscript.
             </p>
          </div>
        </div>
        <!--End Submission of Containers for Testing Phase-->
        <!-------------------------------------------------------------------------------------------->
        <!-------------------------------------------------------------------------------------------->

        <!--Start Submission of Manuscripts-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h2>Submission of Manuscripts</h2>
            <hr>
            <p class="text">
              Participants are invited to submit an 8-12 pages manuscript describing the algorithm and results of training and validation phase for consideration in MICCAI BrainLes workshop 2022. Please follow the LNCS template from Springer for your paper available at: https://www.springer.com/us/computer-science/lncs/conference-proceedings-guidelines
            </p>
             <p class="text">
               All papers will undergo a double blind review process. Submit your paper through the CMT submission system (https://cmt3.research.microsoft.com/BrainLes2022/) and make sure you choose “BraTS-Reg2022 Challenge” as the track. Please note that your manuscript will be considered for publication only if you successfully submit the container.
             </p>
          </div>
        </div>
        <!--End Submission of Manuscripts-->
        <!-------------------------------------------------------------------------------------------->
        <!-------------------------------------------------------------------------------------------->
        
        <!--Start People-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h2>People</h2>     
            <ul>
              <li>
                <p class="text-small-margin">
                  Bhakti Baheti (University of Pennsylvania)
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  Diana Waldmannstetter (University of Zurich and Technical University of Munich)
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                 Satrajit Chakrabarty (Washington University in Saint Louis)
                </p>
              </li>
             <li>
              <p class="text-small-margin">
                Hamed Akbari (University of Pennsylvania)
              </p>
            </li>
            <li>
              <p class="text-small-margin">
                Aristeidis Sotiras (Washington University in Saint Louis)
              </p>
            </li>
            <li>
              <p class="text-small-margin">
               Bjoern Menze (University of Zurich and Technical University of Munich)
              </p>
            </li>
            <li>
              <p class="text-small-margin">
                Spyridon Bakas (University of Pennsylvania) 
              </p>
            </li>
            </ul>

            <ul>
              <li>
                <p class="text-small-margin">
                  Michel Bilello (University of Pennsylvania)
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  Benedikt Wiestler (Technical University of Munich)
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                 Julian Schwarting (Technical University of Munich) 
                </p>
              </li>
             <li>
              <p class="text-small-margin">
                Syed Abidi (Washington University in Saint Louis)
              </p>
            </li>
            <li>
              <p class="text-small-margin">
                Mina Mousa (Washington University in Saint Louis)
              </p>
            </li>
            <li>
              <p class="text-small-margin">
               Evan Calabrese (University of California San Francisco)
              </p>
            </li>
            <li>
              <p class="text-small-margin">
                Jeffrey Rudie (University of California San Francisco)
              </p>
            </li>
            </ul>

            
          </div>
        </div>
        <!--End People-->
        <!-------------------------------------------------------------------------------------------->
        <!-------------------------------------------------------------------------------------------->
       
        <!--Start Grants-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width grants">
            <h2>List of Funded Grants</h2>
            <hr>
            <!-------------------------------------------------------------------------------------------->
            <div>
              <h3>AI4Intelligence: from Multimodal Data to Trustworthy Evidence in Court</h3>
              <ul>
                <li>Primarily funded by NWO and co-funded by others (€1.5M), 09/2022 - 09/2028</li>
                <li>
                  Law enforcement has to process huge amounts of data derived from online platforms, digital marketplaces, or communication services, where AI can serve as a solution.
                  In AI4Intelligence we allow AI tool development, the use of these tools by investigators, and legal regulations to go hand in hand so that investigations can lead to trustworthy evidence that is admissible in court.
                </li>
                <li>Partners: Nationale Politie, NFI, Sustainable Rescue Foundation, TNO, Microsoft, SynerScope BV, CFLW Cyber Strategies, DuckDuck0Goose, ZiuZ Forensics BV, BG.legal, The Hague Court of Appeal, Innovation Team Testlab OM</li>
                <li><a href="https://ivi.uva.nl/content/news/2022/05/ai4intelligence-project-granted.html?origin=AbIBW%2F3BT%2FqqidVpq41UEg">Link to the News</a></li>
              </ul>
            </div>
            <!-------------------------------------------------------------------------------------------->
            <div>
              <h3>AI4FILM</h3>
              <ul>
                <li>Funded by ClickNL (€387K), 09/2022 - 09/2026</li>
                <li>
                  With this project, we aim to develop novel AI techniques that are tailored to film by learning from analysis, production practice, and theory.
                </li>
                <li>Partners: <a href="https://www.kasparai.com/">Kaspar</a></li>
              </ul>
            </div>
            <!-------------------------------------------------------------------------------------------->
            <div>
              <h3>VisXP: Interactive Visual Exploration of Media Archives</h3>
              <ul>
                <li>Funded by ClickNL (€391K), 02/2021 - 12/2023</li>
                <li>
                  This project aims to make AI technology widely applicable within media archives based on interactive learning interfaces that enable users to explore the data visually.
                  This requires research into combining the different types of data sources within archives, both for analysis and for displaying and visualizing with an interface.
                </li>
                <li>Partners: <a href="https://www.beeldengeluid.nl/">The Netherlands Institute for Sound & Vision</a>, <a href="https://www.rtl.nl/">RTL Nederland</a></li>
                <li><a href="https://www.clicknl.nl/en/case/pps-projects-visxp/">Project Link</a></li>
              </ul>
            </div>
            <!-------------------------------------------------------------------------------------------->
          </div>
        </div>
        <!--End Grants-->
        <!-------------------------------------------------------------------------------------------->
        <!-------------------------------------------------------------------------------------------->
        <!--Start Demo-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h2>List of Demos</h2>
            <hr>
          </div>
        </div>
        <!-------------------------------------------------------------------------------------------->
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img class="image max-width-1000" src="img/demo/panorams.png">
          </div>
          <div class="flex-item flex-item-stretch-6 flex-column">
            <p class="text">
              <span class="highlight-text">PanorAMS: Automatic Annotation for Detecting Objects in Urban Context</span><br>
              The PanorAMS framework involves a method to automatically generate bounding box annotations in geo-referenced panoramic images based on geospatial context information.
              We acquire large-scale (albeit noisy) annotations from open data sources.
              (<a href="https://panorams.inskegroenen.nl/">website link</a>, <a href="https://arxiv.org/abs/2208.14295">paper link</a>)
            </p>
          </div>
        </div>
        <!-------------------------------------------------------------------------------------------->
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img class="image max-width-1000" src="img/demo/tindart.jpg">
          </div>
          <div class="flex-item flex-item-stretch-6 flex-column">
            <p class="text">
              <span class="highlight-text">TindART: A Personal Visual Arts Recommender</span><br>
              TindART is a web-based visual artwork reccomendation system.
              The system has visual analytics controls that allow users to gain a deeper understanding of their art taste and refine their personal recommendation.
              (<a href="https://tindart.net/">website link</a>, <a href="https://dl.acm.org/doi/10.1145/3394171.3414445">paper link</a>)
            </p>
          </div>
        </div>
        <!-------------------------------------------------------------------------------------------->
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img class="image max-width-1000" src="img/demo/omniart.jpg">
          </div>
          <div class="flex-item flex-item-stretch-6 flex-column">
            <p class="text">
              <span class="highlight-text">OmniArt: A Large-scale Artistic Benchmark</span><br>
              OmniArt is a large scale artistic benchmark dataset aggregated from multiple collections around the world.
              It is designed for easy data handling and fast integration with popular deep learning frameworks.
              (<a href="https://vistory-omniart.com/">website link</a>, <a href="https://dl.acm.org/doi/10.1145/3273022">paper link</a>)
            </p>
          </div>
        </div>
        <!-------------------------------------------------------------------------------------------->
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img class="image max-width-1000" src="img/demo/gcnillustrator.jpeg">
          </div>
          <div class="flex-item flex-item-stretch-6 flex-column">
            <p class="text">
              <span class="highlight-text">GCNIllustrator: Illustrating the Effect of Hyperparameters on Graph Convolutional Networks</span><br>
              GCNIllustrator is a visual analytics tool for illustrating the effect of hyperparameters on graph convolutional networks (GCNs).
              It addresses one of the most tedious steps in training GCNs: the choice of hyperparameters and their influence on performance.
              (<a href="https://dl.acm.org/doi/abs/10.1145/3474085.3478566">video demo and paper link</a>)
            </p>
          </div>
        </div>
        <!-------------------------------------------------------------------------------------------->
        <!--Use the following commented block of code to add demos-->
        <!-- <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img class="image max-width-1000" src="img/demo/tindart.jpg">
          </div>
          <div class="flex-item flex-item-stretch-6 flex-column">
            <p class="text">
              <span class="highlight-text">TindART: A Personal Visual Arts Recommender</span><br>
              TindART is a web-based visual artwork reccomendation system.
              The system has visual analytics controls that allow users to gain a deeper understanding of their art taste and refine their personal recommendation.
              (<a href="">website link</a>, <a href="">paper link</a>)
            </p>
          </div>
        </div> -->
        <!--Use the above commented block of code to add demos-->
        <!--End Demo-->
        <!-------------------------------------------------------------------------------------------->
        <!-------------------------------------------------------------------------------------------->
        <!--Start Teaching-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h2>List of Open Source Courses</h2>
            <hr>
          </div>
        </div>
        <!-------------------------------------------------------------------------------------------->
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img class="image max-width-1000" src="img/teaching/data-science.png">
          </div>
          <div class="flex-item flex-item-stretch-6 flex-column">
            <p class="text">
              <span class="highlight-text">Data Science (Third Year of the UvA Bachelor Informatiekunde Program)</span><br>
              This course teaches data science pipelines with three modules in processing structured data, text, and images.
              Course materials involve Jupyter Notebooks for hands-on experiences and lectures that explain theories behind the implementations.
              (<a href="https://multix.io/data-science-book-uva/">website link</a>)
            </p>
          </div>
        </div>
        <!-------------------------------------------------------------------------------------------->
      </div>
    </div>
    <!-------------------------------------------------------------------------------------------->
    <div class="banner">
      <div class="banner-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2>Contact Us</h2>
            <p class="text no-bottom-margin">
              If you have any questions regarding our group and research feel free to contact us via the details provided below.
            </p>
            <p class="text no-bottom-margin">
              Informatics Institute, University of Amsterdam, Science Park 900, 1098 XH Amsterdam, The Netherlands.
            </p>
            <p class="text no-bottom-margin">
              Phone: +31-(0)20-525-7521
            </p>
            <p class="text add-bottom-margin-large">
              Email: m.worring@uva.nl
            </p>
          </div>
        </div>
      </div>
    </div>
    <!-------------------------------------------------------------------------------------------->
  </div>
</body>

</html>
