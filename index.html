<!doctype html>
<html>

<head>
  <title>BraTS-Reg</title>
  <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1">
  <link href="css/frame.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/controls.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/custom.css" media="screen" rel="stylesheet" type="text/css" />
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="js/menu.js"></script>
  <style>
    .menu-index {
      color: rgb(255, 255, 255) !important;
      opacity: 1 !important;
      font-weight: 700 !important;
    }
  </style>
</head>

<body>
  <div class="menu-container"></div>
  <div class="content-container">
    <!-------------------------------------------------------------------------------------------->
    <div class="banner" style="background: url('img/brats-reg1.png') no-repeat center; background-size: cover; height: 260px;">
      <div class="banner-table flex-column">
      </div>
    </div>
    <!-------------------------------------------------------------------------------------------->
    <div class="banner">
      <div class="banner-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin-small">Brain Tumor Sequence Registration (BraTS-Reg) Challenge</h2>
            <p class="text">
              Establishing Correspondence between Pre-Operative and Follow-up MRI
            </p>
          </div>
        </div>
      </div>
    </div>
    <!-------------------------------------------------------------------------------------------->
    <div class="content">
      <div class="content-table flex-column">
        <!-------------------------------------------------------------------------------------------->
        <!--Start background-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h2>Background</h2>
            <hr>
            <p class="text">
              Registration of Magnetic Resonance Imaging (MRI) scans containing pathologies is challenging due to tissue appearance changes, and still an unsolved problem. We organize the first Brain Tumor Sequence Registration (BraTS-Reg) challenge, focusing on estimating correspondences between baseline pre-operative and follow up scans of the same patient diagnosed with a brain glioma. The BraTS-Reg  challenge intends to establish a benchmark environment for deformable registration algorithms. The dataset associated with this challenge comprises de-identified multi-institutional multi-parametric MRI (mpMRI) data, curated for each scan’s size and resolution, according to a common anatomical template. The clinical experts of our team have generated extensive annotations of landmarks points within the scans. The “training data” along with these ground truth annotations will be released to participants to design their registration methods, whereas annotations of the “validation” and “test” data will be withheld by the organizers and used to evaluate the containerized algorithms of the participants. We will conduct the quantitative evaluation of the submitted algorithms using several metrics, such as Median Absolute Error and Robustness.  
            </p>
          </div>
        </div>
        <!--End background-->
        
        <!-------------------------------------------------------------------------------------------->
        <!-------------------------------------------------------------------------------------------->

        <!-------------------------------------------------------------------------------------------->
        <!--Start Important Dates-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h2>Important Dates</h2>
            <hr>
            <p class="text">
              +----------------+--------------------------------------------------------------------------------------------------+
              | 8 Apr 2022     | Registration opens for BraTS-Reg @ MICCAI.                                                       |
              +----------------+--------------------------------------------------------------------------------------------------+
              | 8 Apr 2022     | Training & Validation data release.                                                              |
              +----------------+--------------------------------------------------------------------------------------------------+
              | 8 Apr 2022     | Evaluation Platform goes live!                                                                   |
              +----------------+--------------------------------------------------------------------------------------------------+
              | 2 May 2022     | Leaderboard goes live! (https://www.cbica.upenn.edu/BraTSReg22/)                                 |
              +----------------+--------------------------------------------------------------------------------------------------+
              | 17 July 2022   | Submission of short paper reporting method & results.                                            |
              +----------------+--------------------------------------------------------------------------------------------------+
              | 24 July 2022   | Submission of containerized algorithm.                                                           |
              +----------------+--------------------------------------------------------------------------------------------------+
              | 18 Aug 2022    | Contacting top-ranked teams to prepare oral presentations.                                       |
              +----------------+--------------------------------------------------------------------------------------------------+
              | 18-22 Sep 2022 | Announcement of top-ranked teams at MICCAI.                                                      |
              +----------------+--------------------------------------------------------------------------------------------------+
              | 30 Oct 2022    | Camera-ready submission of extended papers for inclusion in the associated workshop proceedings. |
              +----------------+--------------------------------------------------------------------------------------------------+
              |                                     (All deadlines are for 23:59 Eastern Time)                                    |
              +-------------------------------------------------------------------------------------------------------------------+
            </p>
          </div>
        </div>
        <!--End Important Dates-->
        
        <!-------------------------------------------------------------------------------------------->
        <!-------------------------------------------------------------------------------------------->


        <!-------------------------------------------------------------------------------------------->
        <!--Start Clinical Relevance-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h2>Clinical Relevance</h2>
            <hr>
            <p class="text">
              Registration of baseline pre-operative (treatment-naïve) and follow-up brain tumor MRI scans is challenging, yet a clinically important task for a multitude of reasons. Brain tissue shows heavy deformations induced by the apparent tumor (also known as mass effect) that following its resection are relaxed due to the relieving pressure from the resected tissue. Such deformations affect the whole brain (including the lateral ventricles) and are not limited to the vicinity of the tumor. This is particularly important as the relationship of the tumor to the lateral ventricles and the deformations to the rest of the brain tissue are important factors in prognosis and treatment planning. Further changes in the peritumoral edematous/infiltrated tissue, potential tumor recurrence, as well as treatment related changes, also affect the brain tissue elasticity. The resected tissue/tumor also relates to missing correspondences, and inconsistent intensity profiles between the follow up and the baseline pre-operative scans.  
            </p>
            <p class="text">
              Taking all the above into consideration, finding spatial correspondences between two longitudinal scans of brain tumor patients, i.e., the registration between the baseline pre-operative and follow-up MRI scans, can advance our mechanistic understanding for these tumors. Specifically, for tumor infiltration and potential recurrence, further contributing in the generation of predictive modelling for related pathophysiological processes, but also in understanding biophysical dynamic and plasticity characteristic of brain tissues, as well as for neurosurgical planning.
            </p>            
          </div>
        </div>
        <!--End Clinical Relevance-->
        
        <!-------------------------------------------------------------------------------------------->
        <!-------------------------------------------------------------------------------------------->


        <!-------------------------------------------------------------------------------------------->
        <!--Start Task & Evaluation-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h2>Task & Evaluation</h2>
            <hr>
            <p class="text">
              The registration between pre-operative and follow-up MRI scans of brain glioma patients, is important yet challenging task. In this challenge, participants are invited to develop deformable image registration algorithm by using the provided clinically acquired training data and the annotations done by our expert clinical neuroradiologists.
            </p>
            <p class="text">
              The evaluation of the registration between the two scans will be based on manually seeded landmarks (ground truth) in both the pre-operative and the follow-up scans. The performance will be quantitatively evaluated in terms of Median Absolute Error (MAE) and Robustness.
            </p>
          </div>
        </div>
        <!--End Task & Evaluation-->
        
        <!-------------------------------------------------------------------------------------------->
        <!-------------------------------------------------------------------------------------------->


        <!-------------------------------------------------------------------------------------------->
        <!--Start Data-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h2>Data</h2>
            <hr>
            <p class="text">
              We have identified, curated, and pre-processed retrospective multi-institutional data. The data comprises of pairs of pre-operative baseline and follow-up MRI brain scans (each pair being of the same patient) diagnosed and treated for glioma. The exact multi-parametric MRI (mpMRI) sequences of each timepoint are i) native (T1) and ii) contrastenhancedT1-weighted (T1-CE), iii) T2-weighted and iv) T2Fluid Attenuated Inversion Recovery (FLAIR).
            </p>
            <p class="text">
              In training phase, participants will be provided with the ground truth annotations along with the MRI data. The ground truth consists of location of some unique landmark points found in baseline scan and their corresponding locations in follow-up scan to develop the deformable registration algorithms.
            </p>
            <p class="text">
              These landmarks are defined on anatomical markers such as blood vessel bifurcations, the anatomical shape of the cortex, and anatomical landmarks of the midline of the brain. The total number of landmarks vary from case to case and across all cases in the range of 6-50 per scan.
            </p>
            <p class="text">
              The validation data will be provided to the participants as scan pairs of baseline and follow-up with landmarks provided only for the follow-up scan. Participants will submit coordinates of warped landmark locations in the baseline scan. Also, they will be called to upload their method in a containerized way for evaluation on testing data.
            </p>
          </div>
        </div>
        <!--End Data-->
        
        <!-------------------------------------------------------------------------------------------->
        <!-------------------------------------------------------------------------------------------->


        <!-------------------------------------------------------------------------------------------->
        <!--Start Participation Timeline Summary-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h2>Participation Timeline Summary</h2>
            <hr>
            <p class="text">
              Register for the BraTS-Reg challenge, to get access to the NIFTI, skull-stripped, and annotated training data. An independent set of validation scans will be made available to the participants, with the intention to allow them assess the generalizability of their methods on unseen data, via the official evaluation platform. Note that the training data will include its corresponding ground truth, whereas the validation data will have its ground truth data held by the organizers at all times. 
            </p>
            <p class="text">
              Participants will have to evaluate their methods on the training and validation datasets, and submit short paper describing their method and results. The organizers will review the paper for sufficient details required to understand and reproduce the algorithm. The challenge participants will be given a chance with the option to extend their individual papers, and hence publish their methods in Springer LNCS proceedings. 
            </p>
            <p class="text">
              Participants will need to submit their method in a containerized form. Note that only participants that have submitted a short paper will be considered in the testing phase. The BraTS-Reg test data will not be made available to the participating teams at any point during the lifecycle of this challenge. We will communicate more details on how to create the expected containers soon. 
            </p>
            <p class="text">
              The top-ranked participants of the testing phase will be contacted by August to prepare slides for orally presenting their method in MICCAI 2022.  The final results of the challenge will also be reported at MICCAI. 
            </p>
            <p class="text">
              Finally, we intend to coordinate a journal meta-analysis manuscript in one of the reputed journals in the domain by extending the preprint appropriately to describe the challenge design, data, clinical relevance, and summarizing the results and insights of the challenge. 
            </p>
          </div>
        </div>
        <!--End Participation Timeline Summary-->
        <!-------------------------------------------------------------------------------------------->
        <!-------------------------------------------------------------------------------------------->
        <!--Start Alumni-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h2>Alumni</h2>
            <hr>
          </div>
        </div>
        <!-------------------------------------------------------------------------------------------->
        <div class="gallery add-top-margin-small add-bottom-margin-small" id="alumni">
          <!-------------------------------------------------------------------------------------------->
          <div class="flex-column">
            <img src="img/people/sadaf.png">
            <h3>Sadaf Gulshad</h3>
            <div>Previous Postdoctoral Researcher</div>
            <div>
              <a href="https://sites.google.com/view/sadafgulshad/">Website</a>
              &nbsp;|&nbsp;
              <a href="https://www.linkedin.com/in/sadaf-gulshad-6825b636/">LinkedIn</a>
            </div>
            <div>&nbsp;</div>
          </div>
          <!-------------------------------------------------------------------------------------------->
          <div class="flex-column">
            <img src="img/people/inske.jpg">
            <h3>Inske Groenen</h3>
            <div>Previous PhD Student</div>
            <div>
              <a href="https://scholar.google.com/citations?user=EPquzqwAAAAJ&hl=nl&oi=ao">Scholar</a>
              &nbsp;|&nbsp;
              <a href="https://www.linkedin.com/in/inske-groenen/">LinkedIn</a>
            </div>
            <div>&nbsp;</div>
          </div>
          <!-------------------------------------------------------------------------------------------->
          <div class="flex-column">
            <img src="img/people/gjorgji.jpeg">
            <h3>Gjorgji Strezoski</h3>
            <div>Previous PhD Student</div>
            <div>
              <a href="https://scholar.google.nl/citations?user=rRqGXOoAAAAJ&hl=en">Scholar</a>
            </div>
          </div>
          <!-------------------------------------------------------------------------------------------->
          <div class="flex-column">
            <img src="img/people/devanshu.jpg">
            <h3>Devanshu Arya</h3>
            <div>Previous PhD Student</div>
            <div>
              <a href="https://devanshuarya.github.io/">Website</a>
            </div>
          </div>
          <!-------------------------------------------------------------------------------------------->
          <div class="flex-column">
            <img src="img/people/javier.jpeg">
            <h3>Javier Lloret Pardo</h3>
            <div>Previous PhD Student</div>
            <div>
              <a href="https://javierlloret.com/">Website</a>
            </div>
          </div>
          <!-------------------------------------------------------------------------------------------->
          <!--Use the following commented block code to add alumni-->
          <!-- <div class="flex-column">
            <img src="img/people/person.png">
            <h3></h3>
            <div></div>
            <div>
              <a href="">Website</a>
            </div>
          </div> -->
          <!--Use the above commented block code to add alumni-->
        </div>
        <!--End Alumni-->
        <!-------------------------------------------------------------------------------------------->
        <!-------------------------------------------------------------------------------------------->
        <!--Start ICAI Labs-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h2>Paticipation in ICAI Labs</h2>
            <hr>
            <p class="text no-bottom-margin">
              MultiX participates in the following labs of the Innovation Center for Artificial Intelligence:
            </p>
            <ul>
              <li>
                <p class="text-small-margin">
                  The <a href="https://ivi.fnwi.uva.nl/aimlab/">AIM Lab (AI for Medical Imaging)</a> is a collaborative initiative of the Inception Institute of Artificial Intelligence from the United Arab Emirates and the University of Amsterdam.
                  It focuses on using artificial intelligence for medical image recognition.
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  The <a href="https://icai.ai/police-lab-ai/">National Police Lab AI</a> is a collaborative initiative of the Dutch Police, Utrecht University, University of Amsterdam, and TU Delft.
                  They aim to develop state-of-the-art AI techniques to improve safety in the Netherlands in a socially, legally, and ethically responsible way.
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  The <a href="https://ai4forensics.github.io/">AI4Forensics Lab</a> is a collaborative initiative of the Netherlands Forensic Institute and the University of Amsterdam.
                  Our focus is on the study and development of artificial intelligence for forensic purposes.
                </p>
              </li>
            </ul>
          </div>
        </div>
        <!--End ICAI Labs-->
        <!-------------------------------------------------------------------------------------------->
        <!-------------------------------------------------------------------------------------------->
        <!--Start Grants-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width grants">
            <h2>List of Funded Grants</h2>
            <hr>
            <!-------------------------------------------------------------------------------------------->
            <div>
              <h3>AI4Intelligence: from Multimodal Data to Trustworthy Evidence in Court</h3>
              <ul>
                <li>Primarily funded by NWO and co-funded by others (€1.5M), 09/2022 - 09/2028</li>
                <li>
                  Law enforcement has to process huge amounts of data derived from online platforms, digital marketplaces, or communication services, where AI can serve as a solution.
                  In AI4Intelligence we allow AI tool development, the use of these tools by investigators, and legal regulations to go hand in hand so that investigations can lead to trustworthy evidence that is admissible in court.
                </li>
                <li>Partners: Nationale Politie, NFI, Sustainable Rescue Foundation, TNO, Microsoft, SynerScope BV, CFLW Cyber Strategies, DuckDuck0Goose, ZiuZ Forensics BV, BG.legal, The Hague Court of Appeal, Innovation Team Testlab OM</li>
                <li><a href="https://ivi.uva.nl/content/news/2022/05/ai4intelligence-project-granted.html?origin=AbIBW%2F3BT%2FqqidVpq41UEg">Link to the News</a></li>
              </ul>
            </div>
            <!-------------------------------------------------------------------------------------------->
            <div>
              <h3>AI4FILM</h3>
              <ul>
                <li>Funded by ClickNL (€387K), 09/2022 - 09/2026</li>
                <li>
                  With this project, we aim to develop novel AI techniques that are tailored to film by learning from analysis, production practice, and theory.
                </li>
                <li>Partners: <a href="https://www.kasparai.com/">Kaspar</a></li>
              </ul>
            </div>
            <!-------------------------------------------------------------------------------------------->
            <div>
              <h3>VisXP: Interactive Visual Exploration of Media Archives</h3>
              <ul>
                <li>Funded by ClickNL (€391K), 02/2021 - 12/2023</li>
                <li>
                  This project aims to make AI technology widely applicable within media archives based on interactive learning interfaces that enable users to explore the data visually.
                  This requires research into combining the different types of data sources within archives, both for analysis and for displaying and visualizing with an interface.
                </li>
                <li>Partners: <a href="https://www.beeldengeluid.nl/">The Netherlands Institute for Sound & Vision</a>, <a href="https://www.rtl.nl/">RTL Nederland</a></li>
                <li><a href="https://www.clicknl.nl/en/case/pps-projects-visxp/">Project Link</a></li>
              </ul>
            </div>
            <!-------------------------------------------------------------------------------------------->
          </div>
        </div>
        <!--End Grants-->
        <!-------------------------------------------------------------------------------------------->
        <!-------------------------------------------------------------------------------------------->
        <!--Start Demo-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h2>List of Demos</h2>
            <hr>
          </div>
        </div>
        <!-------------------------------------------------------------------------------------------->
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img class="image max-width-1000" src="img/demo/panorams.png">
          </div>
          <div class="flex-item flex-item-stretch-6 flex-column">
            <p class="text">
              <span class="highlight-text">PanorAMS: Automatic Annotation for Detecting Objects in Urban Context</span><br>
              The PanorAMS framework involves a method to automatically generate bounding box annotations in geo-referenced panoramic images based on geospatial context information.
              We acquire large-scale (albeit noisy) annotations from open data sources.
              (<a href="https://panorams.inskegroenen.nl/">website link</a>, <a href="https://arxiv.org/abs/2208.14295">paper link</a>)
            </p>
          </div>
        </div>
        <!-------------------------------------------------------------------------------------------->
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img class="image max-width-1000" src="img/demo/tindart.jpg">
          </div>
          <div class="flex-item flex-item-stretch-6 flex-column">
            <p class="text">
              <span class="highlight-text">TindART: A Personal Visual Arts Recommender</span><br>
              TindART is a web-based visual artwork reccomendation system.
              The system has visual analytics controls that allow users to gain a deeper understanding of their art taste and refine their personal recommendation.
              (<a href="https://tindart.net/">website link</a>, <a href="https://dl.acm.org/doi/10.1145/3394171.3414445">paper link</a>)
            </p>
          </div>
        </div>
        <!-------------------------------------------------------------------------------------------->
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img class="image max-width-1000" src="img/demo/omniart.jpg">
          </div>
          <div class="flex-item flex-item-stretch-6 flex-column">
            <p class="text">
              <span class="highlight-text">OmniArt: A Large-scale Artistic Benchmark</span><br>
              OmniArt is a large scale artistic benchmark dataset aggregated from multiple collections around the world.
              It is designed for easy data handling and fast integration with popular deep learning frameworks.
              (<a href="https://vistory-omniart.com/">website link</a>, <a href="https://dl.acm.org/doi/10.1145/3273022">paper link</a>)
            </p>
          </div>
        </div>
        <!-------------------------------------------------------------------------------------------->
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img class="image max-width-1000" src="img/demo/gcnillustrator.jpeg">
          </div>
          <div class="flex-item flex-item-stretch-6 flex-column">
            <p class="text">
              <span class="highlight-text">GCNIllustrator: Illustrating the Effect of Hyperparameters on Graph Convolutional Networks</span><br>
              GCNIllustrator is a visual analytics tool for illustrating the effect of hyperparameters on graph convolutional networks (GCNs).
              It addresses one of the most tedious steps in training GCNs: the choice of hyperparameters and their influence on performance.
              (<a href="https://dl.acm.org/doi/abs/10.1145/3474085.3478566">video demo and paper link</a>)
            </p>
          </div>
        </div>
        <!-------------------------------------------------------------------------------------------->
        <!--Use the following commented block of code to add demos-->
        <!-- <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img class="image max-width-1000" src="img/demo/tindart.jpg">
          </div>
          <div class="flex-item flex-item-stretch-6 flex-column">
            <p class="text">
              <span class="highlight-text">TindART: A Personal Visual Arts Recommender</span><br>
              TindART is a web-based visual artwork reccomendation system.
              The system has visual analytics controls that allow users to gain a deeper understanding of their art taste and refine their personal recommendation.
              (<a href="">website link</a>, <a href="">paper link</a>)
            </p>
          </div>
        </div> -->
        <!--Use the above commented block of code to add demos-->
        <!--End Demo-->
        <!-------------------------------------------------------------------------------------------->
        <!-------------------------------------------------------------------------------------------->
        <!--Start Teaching-->
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h2>List of Open Source Courses</h2>
            <hr>
          </div>
        </div>
        <!-------------------------------------------------------------------------------------------->
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img class="image max-width-1000" src="img/teaching/data-science.png">
          </div>
          <div class="flex-item flex-item-stretch-6 flex-column">
            <p class="text">
              <span class="highlight-text">Data Science (Third Year of the UvA Bachelor Informatiekunde Program)</span><br>
              This course teaches data science pipelines with three modules in processing structured data, text, and images.
              Course materials involve Jupyter Notebooks for hands-on experiences and lectures that explain theories behind the implementations.
              (<a href="https://multix.io/data-science-book-uva/">website link</a>)
            </p>
          </div>
        </div>
        <!-------------------------------------------------------------------------------------------->
      </div>
    </div>
    <!-------------------------------------------------------------------------------------------->
    <div class="banner">
      <div class="banner-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2>Contact Us</h2>
            <p class="text no-bottom-margin">
              If you have any questions regarding our group and research feel free to contact us via the details provided below.
            </p>
            <p class="text no-bottom-margin">
              Informatics Institute, University of Amsterdam, Science Park 900, 1098 XH Amsterdam, The Netherlands.
            </p>
            <p class="text no-bottom-margin">
              Phone: +31-(0)20-525-7521
            </p>
            <p class="text add-bottom-margin-large">
              Email: m.worring@uva.nl
            </p>
          </div>
        </div>
      </div>
    </div>
    <!-------------------------------------------------------------------------------------------->
  </div>
</body>

</html>
